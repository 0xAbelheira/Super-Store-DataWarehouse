{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the MySQL database using .env file\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from mysql.connector import Error\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version 9.2.0\n",
      "Connected to database: superstore_dw\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = mysql.connector.connect(\n",
    "        host=os.getenv(\"DB_HOST\"),\n",
    "        database=os.getenv(\"DB_NAME\"),\n",
    "        user=os.getenv(\"DB_USER\"),\n",
    "        password=os.getenv(\"DB_PASSWORD\"),\n",
    "    )\n",
    "\n",
    "    if connection.is_connected():\n",
    "        db_info = connection.get_server_info()\n",
    "        print(f\"Connected to MySQL Server version {db_info}\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SELECT DATABASE();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(f\"Connected to database: {record[0]}\")\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit']\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Row ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Order ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Order Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ship Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ship Mode",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Customer ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Customer Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Segment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "State",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Postal Code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Product ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sub-Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Product Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Quantity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Discount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Profit",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "42c55c27-e1d1-4ec9-b961-6148f3c4fcdc",
       "rows": [
        [
         "0",
         "1",
         "CA-2016-152156",
         "11/8/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         "42420",
         "South",
         "FUR-BO-10001798",
         "Furniture",
         "Bookcases",
         "Bush Somerset Collection Bookcase",
         "261.96",
         "2",
         "0.0",
         "41.9136"
        ],
        [
         "1",
         "2",
         "CA-2016-152156",
         "11/8/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         "42420",
         "South",
         "FUR-CH-10000454",
         "Furniture",
         "Chairs",
         "Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back",
         "731.94",
         "3",
         "0.0",
         "219.582"
        ],
        [
         "2",
         "3",
         "CA-2016-138688",
         "6/12/2016",
         "6/16/2016",
         "Second Class",
         "DV-13045",
         "Darrin Van Huff",
         "Corporate",
         "United States",
         "Los Angeles",
         "California",
         "90036",
         "West",
         "OFF-LA-10000240",
         "Office Supplies",
         "Labels",
         "Self-Adhesive Address Labels for Typewriters by Universal",
         "14.62",
         "2",
         "0.0",
         "6.8714"
        ],
        [
         "3",
         "4",
         "US-2015-108966",
         "10/11/2015",
         "10/18/2015",
         "Standard Class",
         "SO-20335",
         "Sean O'Donnell",
         "Consumer",
         "United States",
         "Fort Lauderdale",
         "Florida",
         "33311",
         "South",
         "FUR-TA-10000577",
         "Furniture",
         "Tables",
         "Bretford CR4500 Series Slim Rectangular Table",
         "957.5775",
         "5",
         "0.45",
         "-383.031"
        ],
        [
         "4",
         "5",
         "US-2015-108966",
         "10/11/2015",
         "10/18/2015",
         "Standard Class",
         "SO-20335",
         "Sean O'Donnell",
         "Consumer",
         "United States",
         "Fort Lauderdale",
         "Florida",
         "33311",
         "South",
         "OFF-ST-10000760",
         "Office Supplies",
         "Storage",
         "Eldon Fold 'N Roll Cart System",
         "22.368",
         "2",
         "0.2",
         "2.5164"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>11/8/2016</td>\n",
       "      <td>11/11/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>219.5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2016-138688</td>\n",
       "      <td>6/12/2016</td>\n",
       "      <td>6/16/2016</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-383.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>10/11/2015</td>\n",
       "      <td>10/18/2015</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.5164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "1       2  CA-2016-152156   11/8/2016  11/11/2016    Second Class    CG-12520   \n",
       "2       3  CA-2016-138688   6/12/2016   6/16/2016    Second Class    DV-13045   \n",
       "3       4  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "4       5  US-2015-108966  10/11/2015  10/18/2015  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City  ...  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "\n",
       "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
       "0       42420   South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1       42420   South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2       90036    West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3       33311   South  FUR-TA-10000577        Furniture       Tables   \n",
       "4       33311   South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  Quantity  \\\n",
       "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
       "\n",
       "   Discount    Profit  \n",
       "0      0.00   41.9136  \n",
       "1      0.00  219.5820  \n",
       "2      0.00    6.8714  \n",
       "3      0.45 -383.0310  \n",
       "4      0.20    2.5164  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Sample - Superstore.csv\", encoding=\"windows-1252\")\n",
    "\n",
    "# Display the first few rows and column names\n",
    "print(\"Column names:\", df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (9994, 21)\n",
      "Found 8 order-product combinations that appear multiple times\n",
      "After merging duplicates, dataset shape: (9986, 21)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_merge_duplicate_products(df):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset to merge rows that have the same product in the same order.\n",
    "\n",
    "    Args:\n",
    "        df: The original dataframe containing Superstore data\n",
    "\n",
    "    Returns:\n",
    "        Preprocessed dataframe with merged duplicate product entries\n",
    "    \"\"\"\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # Identify the columns we need to group by to find duplicate order-product combinations\n",
    "    # These are the columns that should uniquely identify an order item\n",
    "    group_cols = [\"Order ID\", \"Product ID\"]\n",
    "\n",
    "    # Count how many times each order-product combination appears\n",
    "    combination_counts = (\n",
    "        processed_df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "    )\n",
    "    duplicates = combination_counts[combination_counts[\"count\"] > 1]\n",
    "\n",
    "    print(\n",
    "        f\"Found {len(duplicates)} order-product combinations that appear multiple times\"\n",
    "    )\n",
    "\n",
    "    if len(duplicates) > 0:\n",
    "        # Create a new dataframe to hold our merged results\n",
    "        merged_rows = []\n",
    "\n",
    "        # Process each group of duplicate order-product combinations\n",
    "        for _, row in duplicates.iterrows():\n",
    "            order_id = row[\"Order ID\"]\n",
    "            product_id = row[\"Product ID\"]\n",
    "\n",
    "            # Get all rows for this order-product combination\n",
    "            filter_condition = (processed_df[\"Order ID\"] == order_id) & (\n",
    "                processed_df[\"Product ID\"] == product_id\n",
    "            )\n",
    "            duplicate_rows = processed_df[filter_condition]\n",
    "\n",
    "            # Take the first row as our template\n",
    "            merged_row = duplicate_rows.iloc[0].copy()\n",
    "\n",
    "            # Calculate aggregated values\n",
    "            total_quantity = duplicate_rows[\"Quantity\"].sum()\n",
    "            total_sales = duplicate_rows[\"Sales\"].sum()\n",
    "\n",
    "            # Calculate weighted discount\n",
    "            # Weight each discount by its proportion of the total quantity\n",
    "            weighted_discount = (\n",
    "                duplicate_rows[\"Discount\"] * duplicate_rows[\"Quantity\"] / total_quantity\n",
    "            ).sum()\n",
    "\n",
    "            # Calculate profit\n",
    "            total_profit = duplicate_rows[\"Profit\"].sum()\n",
    "\n",
    "            # Update the values in our merged row\n",
    "            merged_row[\"Quantity\"] = total_quantity\n",
    "            merged_row[\"Sales\"] = total_sales\n",
    "            merged_row[\"Discount\"] = weighted_discount\n",
    "            merged_row[\"Profit\"] = total_profit\n",
    "\n",
    "            merged_rows.append(merged_row)\n",
    "\n",
    "            # Remove the duplicate rows from our processed dataframe\n",
    "            processed_df = processed_df[~filter_condition]\n",
    "\n",
    "        # Add the merged rows back to the dataframe\n",
    "        merged_df = pd.DataFrame(merged_rows)\n",
    "        processed_df = pd.concat([processed_df, merged_df], ignore_index=True)\n",
    "\n",
    "        print(f\"After merging duplicates, dataset shape: {processed_df.shape}\")\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df = preprocess_merge_duplicate_products(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dimension Tables ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dimension tables...\n",
      "Loaded 1434 records into Calendar dimension\n",
      "Loaded 49 records into CalendarMonth dimension\n",
      "Loaded 793 records into Customer dimension\n",
      "Loaded 4 records into Region dimension\n",
      "Loaded 49 records into State dimension\n",
      "Loaded 632 records into Location dimension\n",
      "Loaded 3 records into Category dimension\n",
      "Loaded 1894 records into Product dimension\n",
      "All dimension tables loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_level_mappings(df):\n",
    "    \"\"\"Create mappings for level keys\"\"\"\n",
    "    # Create sub-category ID mapping\n",
    "    sub_categories = df[\"Sub-Category\"].drop_duplicates().reset_index(drop=True)\n",
    "    sub_category_mapping = {\n",
    "        sub_cat: idx + 1 for idx, sub_cat in enumerate(sub_categories)\n",
    "    }\n",
    "\n",
    "    # Create country ID mapping\n",
    "    countries = df[\"Country\"].drop_duplicates().reset_index(drop=True)\n",
    "    country_mapping = {country: idx + 1 for idx, country in enumerate(countries)}\n",
    "\n",
    "    # Create city ID mapping\n",
    "    city_states = df[[\"City\", \"State\"]].drop_duplicates().reset_index(drop=True)\n",
    "    city_mapping = {}\n",
    "    for idx, (_, row) in enumerate(city_states.iterrows(), 1):\n",
    "        city_mapping[(row[\"City\"], row[\"State\"])] = idx\n",
    "\n",
    "    return {\n",
    "        \"sub_category\": sub_category_mapping,\n",
    "        \"country\": country_mapping,\n",
    "        \"city\": city_mapping,\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to load data into Calendar dimension table\n",
    "def load_calendar_dimension(connection, df):\n",
    "    # Extract unique dates from Order Date and Ship Date\n",
    "    order_dates = pd.to_datetime(df[\"Order Date\"]).dt.date.unique()\n",
    "    ship_dates = pd.to_datetime(df[\"Ship Date\"]).dt.date.unique()\n",
    "    all_dates = sorted(set(order_dates) | set(ship_dates))\n",
    "\n",
    "    # Create year level mapping (sequential IDs for each year)\n",
    "    years = sorted(set([d.year for d in all_dates]))\n",
    "    year_mapping = {year: idx for idx, year in enumerate(years, 1)}\n",
    "\n",
    "    # Create calendar dataframe\n",
    "    calendar_data = []\n",
    "    for date in all_dates:\n",
    "        dt = datetime.combine(date, datetime.min.time())\n",
    "        calendar_data.append(\n",
    "            {\n",
    "                \"full_date\": date,\n",
    "                \"year_id\": year_mapping[\n",
    "                    dt.year\n",
    "                ],  # Use mapped year_id instead of actual year\n",
    "                \"year_number\": dt.year,  # Keep actual year as year_number\n",
    "                \"month_id\": dt.month,\n",
    "                \"month_number\": dt.month,\n",
    "                \"month_name\": dt.strftime(\"%B\"),\n",
    "                \"day_id\": dt.day,\n",
    "                \"day_number\": dt.day,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    calendar_df = pd.DataFrame(calendar_data)\n",
    "\n",
    "    # Insert data into Calendar table\n",
    "    cursor = connection.cursor()\n",
    "    for _, row in calendar_df.iterrows():\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Calendar (full_date, year_id, year_number, month_id, month_number, month_name, day_id, day_number)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query,\n",
    "            (\n",
    "                row[\"full_date\"],\n",
    "                row[\"year_id\"],\n",
    "                row[\"year_number\"],\n",
    "                row[\"month_id\"],\n",
    "                row[\"month_number\"],\n",
    "                row[\"month_name\"],\n",
    "                row[\"day_id\"],\n",
    "                row[\"day_number\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(calendar_df)} records into Calendar dimension\")\n",
    "\n",
    "    # Now populate CalendarMonth table with unique year-month combinations\n",
    "    month_data = calendar_df[\n",
    "        [\"year_id\", \"year_number\", \"month_id\", \"month_number\", \"month_name\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    for _, row in month_data.iterrows():\n",
    "        query = \"\"\"\n",
    "        INSERT INTO CalendarMonth (calendar_month_number, calendar_month_name, year_id, year_number)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query,\n",
    "            (\n",
    "                row[\"month_number\"],\n",
    "                row[\"month_name\"],\n",
    "                row[\"year_id\"],  # Using consistent year_id from the mapping\n",
    "                row[\"year_number\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(month_data)} records into CalendarMonth dimension\")\n",
    "\n",
    "    # Return the year mapping in case needed elsewhere\n",
    "    return year_mapping\n",
    "\n",
    "\n",
    "# Function to load Customer dimension table\n",
    "def load_customer_dimension(connection, df):\n",
    "    # Extract unique customer data\n",
    "    customer_df = df[[\"Customer ID\", \"Customer Name\", \"Segment\"]].drop_duplicates()\n",
    "\n",
    "    # Insert data into Customer table\n",
    "    cursor = connection.cursor()\n",
    "    for _, row in customer_df.iterrows():\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Customer (customer_code, customer_name, segment)\n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query, (row[\"Customer ID\"], row[\"Customer Name\"], row[\"Segment\"])\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(customer_df)} records into Customer dimension\")\n",
    "\n",
    "\n",
    "# Function to load Region, State, and Location dimension tables\n",
    "def load_geography_dimensions(connection, df, level_mappings):\n",
    "    # Extract unique regions and countries\n",
    "    region_df = df[[\"Region\", \"Country\"]].drop_duplicates()\n",
    "\n",
    "    # Insert data into Region table\n",
    "    cursor = connection.cursor()\n",
    "    for _, row in region_df.iterrows():\n",
    "        country_id = level_mappings[\"country\"][row[\"Country\"]]\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Region (region_name, country_id, country_name)\n",
    "        VALUES (%s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (row[\"Region\"], country_id, row[\"Country\"]))\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(region_df)} records into Region dimension\")\n",
    "\n",
    "    # Extract unique state-region combinations\n",
    "    state_df = df[[\"State\", \"Region\", \"Country\"]].drop_duplicates()\n",
    "\n",
    "    # Get region IDs\n",
    "    cursor.execute(\"SELECT region_id, region_name FROM Region\")\n",
    "    region_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "\n",
    "    # Insert data into State table\n",
    "    for _, row in state_df.iterrows():\n",
    "        region_id = region_mapping.get(row[\"Region\"])\n",
    "        country_id = level_mappings[\"country\"][row[\"Country\"]]\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO State (state_name, region_id, region_name, country_id, country_name)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query, (row[\"State\"], region_id, row[\"Region\"], country_id, row[\"Country\"])\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(state_df)} records into State dimension\")\n",
    "\n",
    "    # Now load Location table\n",
    "    location_df = df[[\"Postal Code\", \"City\", \"State\", \"Country\", \"Region\"]].drop_duplicates()\n",
    "\n",
    "    # Get state IDs\n",
    "    cursor.execute(\"SELECT state_id, state_name FROM State\")\n",
    "    state_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "\n",
    "    # Insert data into Location table\n",
    "    for _, row in location_df.iterrows():\n",
    "        country_id = level_mappings[\"country\"][row[\"Country\"]]\n",
    "        state_id = state_mapping.get(row[\"State\"])\n",
    "        city_id = level_mappings[\"city\"][(row[\"City\"], row[\"State\"])]\n",
    "        region_id = region_mapping.get(row[\"Region\"])\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Location (location_code, country_id, country_name, state_id, state_name, city_id, city_name, postal_code, region_id, region_name)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query,\n",
    "            (\n",
    "                row[\"Postal Code\"],\n",
    "                country_id,\n",
    "                row[\"Country\"],\n",
    "                state_id,\n",
    "                row[\"State\"],\n",
    "                city_id,\n",
    "                row[\"City\"],\n",
    "                row[\"Postal Code\"],\n",
    "                region_id,\n",
    "                row[\"Region\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(location_df)} records into Location dimension\")\n",
    "\n",
    "\n",
    "# Function to load Category and Product dimension tables\n",
    "def load_product_dimensions(connection, df, level_mappings):\n",
    "    # Extract unique categories\n",
    "    category_df = df[\"Category\"].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Insert data into Category table\n",
    "    cursor = connection.cursor()\n",
    "    for category in category_df:\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Category (category_name)\n",
    "        VALUES (%s)\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (category,))\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(category_df)} records into Category dimension\")\n",
    "\n",
    "    # Get category IDs for mapping\n",
    "    cursor.execute(\"SELECT category_id, category_name FROM Category\")\n",
    "    category_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "\n",
    "    # Extract unique products\n",
    "    product_df = df[\n",
    "        [\"Product ID\", \"Product Name\", \"Category\", \"Sub-Category\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Insert data into Product table\n",
    "    for _, row in product_df.iterrows():\n",
    "        category_id = category_mapping.get(row[\"Category\"])\n",
    "        sub_category_id = level_mappings[\"sub_category\"][row[\"Sub-Category\"]]\n",
    "\n",
    "        query = \"\"\"\n",
    "        INSERT INTO Product (product_code, product_name, category_id, category_name, sub_category_id, sub_category_name)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        cursor.execute(\n",
    "            query,\n",
    "            (\n",
    "                row[\"Product ID\"],\n",
    "                row[\"Product Name\"],\n",
    "                category_id,\n",
    "                row[\"Category\"],\n",
    "                sub_category_id,\n",
    "                row[\"Sub-Category\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {len(product_df)} records into Product dimension\")\n",
    "\n",
    "\n",
    "# Now execute all of our dimension loading functions\n",
    "try:\n",
    "    # Make sure we're connected\n",
    "    if connection.is_connected():\n",
    "        print(\"Loading dimension tables...\")\n",
    "\n",
    "        # Create level key mappings first\n",
    "        level_mappings = create_level_mappings(df)\n",
    "\n",
    "        load_calendar_dimension(connection, df)\n",
    "        load_customer_dimension(connection, df)\n",
    "        load_geography_dimensions(connection, df, level_mappings)\n",
    "        load_product_dimensions(connection, df, level_mappings)\n",
    "\n",
    "        print(\"All dimension tables loaded successfully!\")\n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fact Tables ETL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Item Fact Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Item fact table...\n",
      "Starting ETL process for Item fact table...\n",
      "Loaded 793 customer mappings\n",
      "Loaded 1862 product mappings\n",
      "Loaded 1434 calendar mappings\n",
      "Loaded 632 location mappings\n",
      "Processed 500 items...\n",
      "Processed 1000 items...\n",
      "Processed 1500 items...\n",
      "Processed 2000 items...\n",
      "Processed 2500 items...\n",
      "Processed 3000 items...\n",
      "Processed 3500 items...\n",
      "Processed 4000 items...\n",
      "Processed 4500 items...\n",
      "Processed 5000 items...\n",
      "Processed 5500 items...\n",
      "Processed 6000 items...\n",
      "Processed 6500 items...\n",
      "Processed 7000 items...\n",
      "Processed 7500 items...\n",
      "Processed 8000 items...\n",
      "Processed 8500 items...\n",
      "Processed 9000 items...\n",
      "Processed 9500 items...\n",
      "Loaded 9986 records into Item fact table\n",
      "Skipped 0 records due to missing dimension keys\n",
      "Item fact table loading complete - 9986 records inserted\n"
     ]
    }
   ],
   "source": [
    "# Function to load Item fact table\n",
    "def load_item_fact_table(connection, df):\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    print(\"Starting ETL process for Item fact table...\")\n",
    "    \n",
    "    # Step 1: Retrieve all necessary dimension keys from the database\n",
    "    # Get customer mappings\n",
    "    cursor.execute(\"SELECT customer_id, customer_code FROM Customer\")\n",
    "    customer_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "    print(f\"Loaded {len(customer_mapping)} customer mappings\")\n",
    "    \n",
    "    # Get product mappings\n",
    "    cursor.execute(\"SELECT product_id, product_code FROM Product\")\n",
    "    product_mapping = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "    print(f\"Loaded {len(product_mapping)} product mappings\")\n",
    "    \n",
    "    # Get calendar mappings for order dates\n",
    "    cursor.execute(\"SELECT calendar_id, full_date FROM Calendar\")\n",
    "    calendar_mapping = {row[1].strftime('%Y-%m-%d'): row[0] for row in cursor.fetchall()}\n",
    "    print(f\"Loaded {len(calendar_mapping)} calendar mappings\")\n",
    "    \n",
    "    # Get location mappings - using postal code and city as the composite key\n",
    "    cursor.execute(\"SELECT location_id, postal_code, city_name FROM Location\")\n",
    "    location_mapping = {(row[1], row[2]): row[0] for row in cursor.fetchall()}\n",
    "    print(f\"Loaded {len(location_mapping)} location mappings\")\n",
    "    \n",
    "    # Step 2: Process each row in the dataframe\n",
    "    item_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            # Format dates for lookup\n",
    "            order_date = pd.to_datetime(row['Order Date']).strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Look up dimension keys\n",
    "            customer_id = customer_mapping.get(row['Customer ID'])\n",
    "            product_id = product_mapping.get(row['Product ID'])\n",
    "            calendar_id = calendar_mapping.get(order_date)\n",
    "            location_key = (str(row['Postal Code']), row['City'])\n",
    "            location_id = location_mapping.get(location_key)\n",
    "            \n",
    "            # Skip if any dimension key is missing\n",
    "            if not all([customer_id, product_id, calendar_id, location_id]):\n",
    "                skipped_count += 1\n",
    "                if skipped_count <= 5:  # Limit the number of error messages\n",
    "                    print(f\"Skipping record due to missing keys - Order ID: {row['Order ID']}, Product: {row['Product Name']}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate fact measures based on the requirements\n",
    "            quantity = int(row['Quantity'])\n",
    "            sales = float(row['Sales'])\n",
    "            discount = float(row['Discount'])\n",
    "            \n",
    "            # Calculate lost_value (difference between full price and discounted price)\n",
    "            # Full price = sales / (1 - discount)\n",
    "            if discount < 1:\n",
    "                full_price = sales / (1 - discount)\n",
    "                lost_value = full_price - sales\n",
    "            else:\n",
    "                lost_value = 0  # Handle edge case of 100% discount\n",
    "                \n",
    "            profit = float(row['Profit'])\n",
    "            \n",
    "            # Insert into fact table\n",
    "            query = \"\"\"\n",
    "            INSERT INTO Item (customer_id, location_id, calendar_id, product_id, \n",
    "                            order_code, quantity, sales, discount, lost_value, profit)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(query, (\n",
    "                customer_id, location_id, calendar_id, product_id,\n",
    "                row['Order ID'], quantity, sales, discount, lost_value, profit\n",
    "            ))\n",
    "            \n",
    "            item_count += 1\n",
    "            \n",
    "            # Commit in batches to improve performance\n",
    "            if item_count % 500 == 0:\n",
    "                connection.commit()\n",
    "                print(f\"Processed {item_count} items...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            print(f\"Row data: {row['Order ID']}, {row['Product Name']}\")\n",
    "    \n",
    "    # Final commit\n",
    "    connection.commit()\n",
    "    print(f\"Loaded {item_count} records into Item fact table\")\n",
    "    print(f\"Skipped {skipped_count} records due to missing dimension keys\")\n",
    "    \n",
    "    return item_count\n",
    "\n",
    "# Execute the loading function\n",
    "try:\n",
    "    # Make sure we're connected\n",
    "    if connection.is_connected():\n",
    "        print(\"Loading Item fact table...\")\n",
    "        item_count = load_item_fact_table(connection, df)\n",
    "        print(f\"Item fact table loading complete - {item_count} records inserted\")\n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 combinations with potential duplicate fact rows\n",
      "No duplicate combinations found\n"
     ]
    }
   ],
   "source": [
    "# Check for rows that would have the same customer_id, product_id, calendar_id and location_id\n",
    "# We need to group by the columns that serve as keys for these dimensions\n",
    "\n",
    "# First, let's standardize the Order Date format\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "\n",
    "# Group by the combination of keys that would map to the same dimension IDs\n",
    "potential_duplicates = (\n",
    "    df.groupby(\n",
    "        [\n",
    "            \"Customer ID\",  # Maps to customer_id\n",
    "            \"Product ID\",  # Maps to product_id\n",
    "            \"Order Date\",  # Maps to calendar_id\n",
    "            \"Postal Code\",  # These together map to location_id\n",
    "            \"City\",  #\n",
    "        ]\n",
    "    )\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Filter to only show combinations that appear more than once\n",
    "duplicates = potential_duplicates[potential_duplicates[\"count\"] > 1]\n",
    "\n",
    "print(f\"Found {len(duplicates)} combinations with potential duplicate fact rows\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nExample duplicate combinations:\")\n",
    "    print(duplicates.head())\n",
    "\n",
    "    # Show detail for the first duplicate combination\n",
    "    if len(duplicates) > 0:\n",
    "        first_dup = duplicates.iloc[0]\n",
    "        detail_filter = (\n",
    "            (df[\"Customer ID\"] == first_dup[\"Customer ID\"])\n",
    "            & (df[\"Product ID\"] == first_dup[\"Product ID\"])\n",
    "            & (df[\"Order Date\"] == first_dup[\"Order Date\"])\n",
    "            & (df[\"Postal Code\"] == first_dup[\"Postal Code\"])\n",
    "            & (df[\"City\"] == first_dup[\"City\"])\n",
    "        )\n",
    "\n",
    "        print(\"\\nDetailed view of the first duplicate combination:\")\n",
    "        duplicate_detail = df[detail_filter][\n",
    "            [\n",
    "                \"Order ID\",\n",
    "                \"Customer ID\",\n",
    "                \"Product ID\",\n",
    "                \"Product Name\",\n",
    "                \"Order Date\",\n",
    "                \"Postal Code\",\n",
    "                \"City\",\n",
    "                \"Quantity\",\n",
    "                \"Sales\",\n",
    "                \"Discount\",\n",
    "            ]\n",
    "        ]\n",
    "        print(duplicate_detail)\n",
    "else:\n",
    "    print(\"No duplicate combinations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers with multiple locations: 780\n",
      "\n",
      "Example of customers with multiple locations:\n",
      "             Postal Code  City  State\n",
      "Customer ID                          \n",
      "AA-10315               5     4      4\n",
      "AA-10375               9     9      8\n",
      "AA-10480               4     4      4\n",
      "AA-10645               6     6      5\n",
      "AB-10015               3     3      3\n",
      "\n",
      "Location details for customer AA-10315:\n",
      "     Customer Name  Postal Code           City       State\n",
      "1155    Alex Avila        55407    Minneapolis   Minnesota\n",
      "1295    Alex Avila        94109  San Francisco  California\n",
      "2223    Alex Avila        94122  San Francisco  California\n",
      "5188    Alex Avila        78664     Round Rock       Texas\n",
      "7456    Alex Avila        10011  New York City    New York\n"
     ]
    }
   ],
   "source": [
    "# Check if it is possible to have one customer with multiple locations in the csv\n",
    "customer_locations = df.groupby(\"Customer ID\").agg(\n",
    "    {\"Postal Code\": \"nunique\", \"City\": \"nunique\", \"State\": \"nunique\"}\n",
    ")\n",
    "\n",
    "# Find customers with multiple locations\n",
    "customers_with_multiple_locations = customer_locations[\n",
    "    (customer_locations[\"Postal Code\"] > 1)\n",
    "    | (customer_locations[\"City\"] > 1)\n",
    "    | (customer_locations[\"State\"] > 1)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of customers with multiple locations: {len(customers_with_multiple_locations)}\"\n",
    ")\n",
    "\n",
    "if len(customers_with_multiple_locations) > 0:\n",
    "    print(\"\\nExample of customers with multiple locations:\")\n",
    "    print(customers_with_multiple_locations.head())\n",
    "\n",
    "    # Get details of a specific customer with multiple locations\n",
    "    example_customer = customers_with_multiple_locations.index[0]\n",
    "    print(f\"\\nLocation details for customer {example_customer}:\")\n",
    "    print(\n",
    "        df[df[\"Customer ID\"] == example_customer][\n",
    "            [\"Customer Name\", \"Postal Code\", \"City\", \"State\"]\n",
    "        ].drop_duplicates()\n",
    "    )\n",
    "else:\n",
    "    print(\"No customers with multiple locations found.\")\n",
    "\n",
    "# check if it is possible to have one location with multiple customers in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 488 postal codes with multiple customers\n",
      "\n",
      "Top postal codes by number of customers:\n",
      "     Postal Code  Number of Customers\n",
      "55         10035                  118\n",
      "54         10024                  115\n",
      "52         10009                  105\n",
      "578        94122                   95\n",
      "53         10011                   92\n",
      "577        94110                   82\n",
      "576        94109                   78\n",
      "620        98105                   74\n",
      "619        98103                   72\n",
      "519        90049                   69\n",
      "\n",
      "Detailed customer breakdown for postal code 10035:\n",
      "     Customer ID     Customer Name           City     State\n",
      "110     CV-12805     Cynthia Voltz  New York City  New York\n",
      "189     MP-17470       Mark Packer  New York City  New York\n",
      "352     JL-15505   Jeremy Lonsdale  New York City  New York\n",
      "542     AS-10135      Adrian Shami  New York City  New York\n",
      "648     AR-10405  Allen Rosenblatt  New York City  New York\n",
      "...          ...               ...            ...       ...\n",
      "9811    JS-15685          Jim Sink  New York City  New York\n",
      "9859    RF-19840   Roy Französisch  New York City  New York\n",
      "9863    JK-15370        Jay Kimmel  New York City  New York\n",
      "9902    JF-15190      Jamie Frazer  New York City  New York\n",
      "9907    DH-13075     Dave Hallsten  New York City  New York\n",
      "\n",
      "[118 rows x 4 columns]\n",
      "\n",
      "Order counts per customer in postal code 10035:\n",
      "    Customer ID     Customer Name  Number of Orders\n",
      "44     GT-14710         Greg Tran                 2\n",
      "28     DO-13435      Denny Ordway                 2\n",
      "71     LW-17215        Luke Weiss                 2\n",
      "76     MH-17785       Maya Herman                 2\n",
      "81     MS-17770  Maxwell Schwartz                 2\n",
      "..          ...               ...               ...\n",
      "111    TB-21625       Trudy Brown                 1\n",
      "113    TG-21310        Toby Gnade                 1\n",
      "114    TH-21550     Tracy Hopkins                 1\n",
      "116    TT-21220   Thomas Thornton                 1\n",
      "117    VP-21760  Victoria Pisteka                 1\n",
      "\n",
      "[118 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count the number of customers in each postal code (focusing on those with multiple customers)\n",
    "postal_code_customer_counts = (\n",
    "    df.groupby(\"Postal Code\")[\"Customer ID\"].nunique().reset_index()\n",
    ")\n",
    "postal_code_customer_counts.columns = [\"Postal Code\", \"Number of Customers\"]\n",
    "\n",
    "# Filter to only show postal codes with multiple customers\n",
    "multiple_customer_postal_codes = postal_code_customer_counts[\n",
    "    postal_code_customer_counts[\"Number of Customers\"] > 1\n",
    "].sort_values(by=\"Number of Customers\", ascending=False)\n",
    "\n",
    "print(\n",
    "    f\"Found {len(multiple_customer_postal_codes)} postal codes with multiple customers\"\n",
    ")\n",
    "\n",
    "if len(multiple_customer_postal_codes) > 0:\n",
    "    print(\"\\nTop postal codes by number of customers:\")\n",
    "    print(multiple_customer_postal_codes.head(10))\n",
    "\n",
    "    # Get detailed breakdown of a specific postal code with the most customers\n",
    "    top_postal_code = multiple_customer_postal_codes.iloc[0][\"Postal Code\"]\n",
    "    print(f\"\\nDetailed customer breakdown for postal code {top_postal_code}:\")\n",
    "    print(\n",
    "        df[df[\"Postal Code\"] == top_postal_code][\n",
    "            [\"Customer ID\", \"Customer Name\", \"City\", \"State\"]\n",
    "        ].drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # Count total orders from each customer in this postal code\n",
    "    print(f\"\\nOrder counts per customer in postal code {top_postal_code}:\")\n",
    "    top_postal_orders = (\n",
    "        df[df[\"Postal Code\"] == top_postal_code]\n",
    "        .groupby([\"Customer ID\", \"Customer Name\"])[\"Order ID\"]\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    top_postal_orders.columns = [\"Customer ID\", \"Customer Name\", \"Number of Orders\"]\n",
    "    print(top_postal_orders.sort_values(by=\"Number of Orders\", ascending=False))\n",
    "else:\n",
    "    print(\"No postal codes with multiple customers found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
